{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":91249,"databundleVersionId":11294684,"isSourceIdPinned":false,"sourceType":"competition"},{"sourceId":224916709,"sourceType":"kernelVersion"},{"sourceId":226382143,"sourceType":"kernelVersion"},{"sourceId":423703,"sourceType":"modelInstanceVersion","isSourceIdPinned":false,"modelInstanceId":345296,"modelId":366583},{"sourceId":423739,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":345328,"modelId":366619}],"dockerImageVersionId":30919,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# BYU Locating Flagellar Motors\n\n## Submission Generation Notebook\n\nThis is the fourth and final notebook in a series for the BYU Locating Bacterial Flagellar Motors 2025 Kaggle challenge. This notebook creates predictions on test data and generates the competition submission file.\n\n### Notebook Series:\n1. **[Parse Data](https://www.kaggle.com/code/andrewjdarley/parse-data)**: Extracting and preparing 2D slices containing motors to make a YOLO dataset\n2. **[Visualize Data](https://www.kaggle.com/code/andrewjdarley/visualize-data)**: Exploratory data analysis and visualization of annotated motor locations\n3. **[Train YOLO](https://www.kaggle.com/code/andrewjdarley/train-yolo)**: Fine tuning an YOLOv8 object detection model on the prepared dataset\n4. **Submission Notebook (Current)**: Running inference and generating submission files \n\n## Important: Offline Execution\nThis notebook is designed to run in an offline environment. The Ultralytics YOLOv8 package has been installed using the offline installation method from [this reference notebook](https://www.kaggle.com/code/itsuki9180/ultralytics-for-offline-install). This implementation was brilliant. I use my own copy as input that works effectively the same as the original.\n\n## About this Notebook\n\nThis submission notebook implements an optimized inference pipeline that:\n\n1. **Model Loading**: Loads the best trained YOLOv8 weights from the training notebook\n2. **GPU Optimization**: Configures CUDA optimizations, half-precision inference, and memory management\n3. **Parallel Processing**: Uses CUDA streams and batch processing for efficient GPU utilization\n4. **3D Detection**: Processes each slice to locate motors\n5. **Non-Maximum Suppression**: Applies 3D NMS to cluster and merge detections across slices\n6. **Submission Generation**: Creates the final CSV file with predicted motor coordinates\n\nThe code includes advanced optimizations like dynamic batch sizing based on available GPU memory, preloading batches while processing the current batch, and GPU profiling to monitor performance. The CONCENTRATION parameter can be adjusted to trade off between processing speed and detection accuracy. The only reason you'd ever modify CONCENTRATION is just to verify submission capability since full submission takes a few hours.","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19"}},{"cell_type":"code","source":"!tar xfvz /kaggle/input/ultralytics-for-offline-install/archive.tar.gz\n!pip install --no-index --find-links=./packages ultralytics\n!rm -rf ./packages","metadata":{"trusted":true,"_kg_hide-input":false,"execution":{"iopub.status.busy":"2025-06-03T19:21:56.339472Z","iopub.execute_input":"2025-06-03T19:21:56.339899Z","iopub.status.idle":"2025-06-03T19:22:22.487953Z","shell.execute_reply.started":"2025-06-03T19:21:56.339869Z","shell.execute_reply":"2025-06-03T19:22:22.486854Z"}},"outputs":[{"name":"stdout","text":"./packages/\n./packages/networkx-3.4.2-py3-none-any.whl\n./packages/fsspec-2025.2.0-py3-none-any.whl\n./packages/python_dateutil-2.9.0.post0-py2.py3-none-any.whl\n./packages/jinja2-3.1.5-py3-none-any.whl\n./packages/pyparsing-3.2.1-py3-none-any.whl\n./packages/charset_normalizer-3.4.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\n./packages/ultralytics_thop-2.0.14-py3-none-any.whl\n./packages/nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl\n./packages/urllib3-2.3.0-py3-none-any.whl\n./packages/nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl\n./packages/pytz-2025.1-py2.py3-none-any.whl\n./packages/MarkupSafe-3.0.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\n./packages/numpy-2.1.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\n./packages/cycler-0.12.1-py3-none-any.whl\n./packages/nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl\n./packages/nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl\n./packages/torchvision-0.21.0-cp310-cp310-manylinux1_x86_64.whl\n./packages/nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl\n./packages/ultralytics-8.3.80-py3-none-any.whl\n./packages/mpmath-1.3.0-py3-none-any.whl\n./packages/kiwisolver-1.4.8-cp310-cp310-manylinux_2_12_x86_64.manylinux2010_x86_64.whl\n./packages/typing_extensions-4.12.2-py3-none-any.whl\n./packages/PyYAML-6.0.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\n./packages/contourpy-1.3.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\n./packages/certifi-2025.1.31-py3-none-any.whl\n./packages/opencv_python-4.11.0.86-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\n./packages/tzdata-2025.1-py2.py3-none-any.whl\n./packages/nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl\n./packages/nvidia_nvtx_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl\n./packages/nvidia_nccl_cu12-2.21.5-py3-none-manylinux2014_x86_64.whl\n./packages/fonttools-4.56.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\n./packages/nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl\n./packages/tqdm-4.67.1-py3-none-any.whl\n./packages/nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl\n./packages/py_cpuinfo-9.0.0-py3-none-any.whl\n./packages/psutil-7.0.0-cp36-abi3-manylinux_2_12_x86_64.manylinux2010_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl\n./packages/pandas-2.2.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\n./packages/torch-2.6.0-cp310-cp310-manylinux1_x86_64.whl\n./packages/nvidia_cusparselt_cu12-0.6.2-py3-none-manylinux2014_x86_64.whl\n./packages/triton-3.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\n./packages/sympy-1.13.1-py3-none-any.whl\n./packages/seaborn-0.13.2-py3-none-any.whl\n./packages/scipy-1.15.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\n./packages/idna-3.10-py3-none-any.whl\n./packages/packaging-24.2-py3-none-any.whl\n./packages/matplotlib-3.10.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\n./packages/nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl\n./packages/nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl\n./packages/requests-2.32.3-py3-none-any.whl\n./packages/six-1.17.0-py2.py3-none-any.whl\n./packages/pillow-11.1.0-cp310-cp310-manylinux_2_28_x86_64.whl\n./packages/filelock-3.17.0-py3-none-any.whl\nLooking in links: ./packages\nRequirement already satisfied: ultralytics in /usr/local/lib/python3.10/dist-packages (8.3.80)\nRequirement already satisfied: numpy<=2.1.1,>=1.23.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (1.26.4)\nRequirement already satisfied: matplotlib>=3.3.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (3.7.5)\nRequirement already satisfied: opencv-python>=4.6.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (4.10.0.84)\nRequirement already satisfied: pillow>=7.1.2 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (11.0.0)\nRequirement already satisfied: pyyaml>=5.3.1 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (6.0.2)\nRequirement already satisfied: requests>=2.23.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (2.32.3)\nRequirement already satisfied: scipy>=1.4.1 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (1.13.1)\nRequirement already satisfied: torch>=1.8.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (2.5.1+cu121)\nRequirement already satisfied: torchvision>=0.9.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (0.20.1+cu121)\nRequirement already satisfied: tqdm>=4.64.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (4.67.1)\nRequirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from ultralytics) (5.9.5)\nRequirement already satisfied: py-cpuinfo in /usr/local/lib/python3.10/dist-packages (from ultralytics) (9.0.0)\nRequirement already satisfied: pandas>=1.1.4 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (2.2.3)\nRequirement already satisfied: seaborn>=0.11.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (0.12.2)\nRequirement already satisfied: ultralytics-thop>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (2.0.14)\nRequirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics) (1.3.1)\nRequirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics) (0.12.1)\nRequirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics) (4.55.3)\nRequirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics) (1.4.7)\nRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics) (24.2)\nRequirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics) (3.2.0)\nRequirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics) (2.9.0.post0)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.10/dist-packages (from numpy<=2.1.1,>=1.23.0->ultralytics) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.10/dist-packages (from numpy<=2.1.1,>=1.23.0->ultralytics) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.10/dist-packages (from numpy<=2.1.1,>=1.23.0->ultralytics) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.10/dist-packages (from numpy<=2.1.1,>=1.23.0->ultralytics) (2025.0.1)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.10/dist-packages (from numpy<=2.1.1,>=1.23.0->ultralytics) (2022.0.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.10/dist-packages (from numpy<=2.1.1,>=1.23.0->ultralytics) (2.4.1)\nRequirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.1.4->ultralytics) (2025.1)\nRequirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.1.4->ultralytics) (2025.1)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.23.0->ultralytics) (3.4.1)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.23.0->ultralytics) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.23.0->ultralytics) (2.3.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.23.0->ultralytics) (2025.1.31)\nRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics) (3.17.0)\nRequirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics) (4.12.2)\nRequirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics) (3.4.2)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics) (3.1.4)\nRequirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics) (2024.12.0)\nRequirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics) (1.13.1)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch>=1.8.0->ultralytics) (1.3.0)\nRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib>=3.3.0->ultralytics) (1.17.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.8.0->ultralytics) (3.0.2)\nRequirement already satisfied: intel-openmp>=2024 in /usr/local/lib/python3.10/dist-packages (from mkl->numpy<=2.1.1,>=1.23.0->ultralytics) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.10/dist-packages (from mkl->numpy<=2.1.1,>=1.23.0->ultralytics) (2022.0.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.10/dist-packages (from tbb==2022.*->mkl->numpy<=2.1.1,>=1.23.0->ultralytics) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.10/dist-packages (from mkl_umath->numpy<=2.1.1,>=1.23.0->ultralytics) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.10/dist-packages (from intel-openmp>=2024->mkl->numpy<=2.1.1,>=1.23.0->ultralytics) (2024.2.0)\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"import os\nimport numpy as np\nimport pandas as pd\nfrom PIL import Image\nimport torch\nimport cv2\nfrom tqdm.notebook import tqdm\nimport threading\nimport time\nfrom contextlib import nullcontext\nfrom concurrent.futures import ThreadPoolExecutor\n\n# Import torchvision components for Faster R-CNN\nimport torchvision\nfrom torchvision.models.detection.faster_rcnn import fasterrcnn_resnet50_fpn\nfrom torchvision.models.detection.faster_rcnn import FastRCNNPredictor\nimport torchvision.transforms as T\n\n# Set random seed for reproducibility\nnp.random.seed(42)\ntorch.manual_seed(42)\n\n# Define paths\ndata_path = \"/kaggle/input/byu-locating-bacterial-flagellar-motors-2025/\"\ntest_dir = os.path.join(data_path, \"test\")\nsubmission_path = \"/kaggle/working/submission.csv\"\n\n# Faster R-CNN model path (adjust if your model is saved elsewhere)\nmodel_path = \"/kaggle/input/test-faster-rcnn/pytorch/default/1/fasterrcnn_motor_detector_2.pth\"\n\n# Detection parameters\nCONFIDENCE_THRESHOLD = 0.45  # Lower threshold to catch more potential motors\nMAX_DETECTIONS_PER_TOMO = 3  # Keep track of top N detections per tomogram\nNMS_IOU_THRESHOLD = 0.2     # Non-maximum suppression threshold for 3D clustering\nCONCENTRATION = 1           # ONLY PROCESS a fraction of slices for speed\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-03T19:22:22.489498Z","iopub.execute_input":"2025-06-03T19:22:22.489744Z","iopub.status.idle":"2025-06-03T19:22:22.497539Z","shell.execute_reply.started":"2025-06-03T19:22:22.489724Z","shell.execute_reply":"2025-06-03T19:22:22.496725Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"# GPU profiling context manager\nclass GPUProfiler:\n    def __init__(self, name):\n        self.name = name\n        self.start_time = None\n        \n    def __enter__(self):\n        if torch.cuda.is_available():\n            torch.cuda.synchronize()\n        self.start_time = time.time()\n        return self\n        \n    def __exit__(self, *args):\n        if torch.cuda.is_available():\n            torch.cuda.synchronize()\n        elapsed = time.time() - self.start_time\n        print(f\"[PROFILE] {self.name}: {elapsed:.3f}s\")\n\n# Check GPU availability and set up optimizations\ndevice = 'cuda:0' if torch.cuda.is_available() else 'cpu'\nBATCH_SIZE = 8  # Default batch size; will adjust if GPU available\n\nif device.startswith('cuda'):\n    # CUDA optimization flags\n    torch.backends.cudnn.benchmark = True\n    torch.backends.cudnn.deterministic = False\n    torch.backends.cuda.matmul.allow_tf32 = True\n    torch.backends.cudnn.allow_tf32 = True\n    \n    # Print GPU info\n    gpu_name = torch.cuda.get_device_name(0)\n    gpu_mem = torch.cuda.get_device_properties(0).total_memory / 1e9  # GB\n    print(f\"Using GPU: {gpu_name} with {gpu_mem:.2f} GB memory\")\n    \n    # Estimate free memory and adjust batch size (approx. 4 images per GB)\n    free_mem = gpu_mem - torch.cuda.memory_allocated(0) / 1e9\n    BATCH_SIZE = max(8, min(32, int(free_mem * 4)))\n    print(f\"Dynamic batch size set to {BATCH_SIZE} based on {free_mem:.2f} GB free memory\")\nelse:\n    print(\"GPU not available, using CPU\")\n    BATCH_SIZE = 4  # Smaller batch size on CPU\n\ndef normalize_slice(slice_data):\n    \"\"\"\n    Normalize slice data using 2nd and 98th percentiles for contrast\n    \"\"\"\n    p2 = np.percentile(slice_data, 2)\n    p98 = np.percentile(slice_data, 98)\n    clipped_data = np.clip(slice_data, p2, p98)\n    normalized = 255 * (clipped_data - p2) / (p98 - p2)\n    return np.uint8(normalized)\n\ndef preload_image_batch(file_paths):\n    \"\"\"Preload a batch of images into CPU memory\"\"\"\n    images = []\n    for path in file_paths:\n        img = cv2.imread(path)\n        if img is None:\n            img = np.array(Image.open(path))\n        images.append(img)\n    return images\n\ndef process_tomogram(tomo_id, model, index=0, total=1):\n    \"\"\"\n    Process a single tomogram folder and return the top motor detection (z, y, x).\n    \"\"\"\n    print(f\"Processing tomogram {tomo_id} ({index}/{total})\")\n    tomo_dir = os.path.join(test_dir, tomo_id)\n    slice_files = sorted([f for f in os.listdir(tomo_dir) if f.endswith('.jpg')])\n    \n    # Apply CONCENTRATION to reduce slice count\n    selected_indices = np.linspace(0, len(slice_files)-1, int(len(slice_files) * CONCENTRATION))\n    selected_indices = np.round(selected_indices).astype(int)\n    slice_files = [slice_files[i] for i in selected_indices]\n    print(f\"Processing {len(slice_files)} out of {len(os.listdir(tomo_dir))} slices \"\n          f\"based on CONCENTRATION={CONCENTRATION}\")\n    \n    all_detections = []  # to store {z, y, x, confidence}\n    \n    # Create CUDA streams if on GPU\n    if device.startswith('cuda'):\n        streams = [torch.cuda.Stream() for _ in range(min(4, BATCH_SIZE))]\n    else:\n        streams = [None]\n    \n    next_batch_thread = None\n    next_batch_images = None\n    \n    # Precompute the torchvision transform: convert PIL image to tensor\n    to_tensor = T.ToTensor()\n    \n    for batch_start in range(0, len(slice_files), BATCH_SIZE):\n        # Wait for previous preload thread to finish\n        if next_batch_thread is not None:\n            next_batch_thread.join()\n            next_batch_images = None\n        \n        batch_end = min(batch_start + BATCH_SIZE, len(slice_files))\n        batch_files = slice_files[batch_start:batch_end]\n        \n        # Preload next batch\n        next_batch_start = batch_end\n        next_batch_end = min(next_batch_start + BATCH_SIZE, len(slice_files))\n        next_batch_files = slice_files[next_batch_start:next_batch_end] if next_batch_start < len(slice_files) else []\n        if next_batch_files:\n            next_batch_paths = [os.path.join(tomo_dir, f) for f in next_batch_files]\n            next_batch_thread = threading.Thread(target=preload_image_batch, args=(next_batch_paths,))\n            next_batch_thread.start()\n        else:\n            next_batch_thread = None\n        \n        # Split this batch across CUDA streams\n        sub_batches = np.array_split(batch_files, len(streams))\n        \n        for i, sub_batch in enumerate(sub_batches):\n            if len(sub_batch) == 0:\n                continue\n            \n            stream = streams[i % len(streams)]\n            with torch.cuda.stream(stream) if (stream and device.startswith('cuda')) else nullcontext():\n                # Prepare images and z-coordinates\n                images_tensor_list = []\n                sub_batch_slice_nums = []\n                for slice_file in sub_batch:\n                    img_path = os.path.join(tomo_dir, slice_file)\n                    \n                    # Read image (grayscale), normalize contrast, convert to RGB\n                    raw = cv2.imread(img_path, cv2.IMREAD_GRAYSCALE)\n                    normalized = normalize_slice(raw)\n                    rgb = cv2.cvtColor(normalized, cv2.COLOR_GRAY2RGB)\n                    pil_img = Image.fromarray(rgb)  # PIL Image\n                    \n                    # Convert to tensor and move to device\n                    img_tensor = to_tensor(pil_img).to(device)\n                    images_tensor_list.append(img_tensor)\n                    \n                    # Extract z-index from filename: e.g., slice_123.jpg â†’ z=123\n                    z_index = int(slice_file.split('_')[1].split('.')[0])\n                    sub_batch_slice_nums.append(z_index)\n                \n                if not images_tensor_list:\n                    continue\n                \n                # Run inference with profiling\n                with GPUProfiler(f\"Inference batch {i+1}/{len(sub_batches)}\"):\n                    model.eval()\n                    with torch.no_grad():\n                        outputs = model(images_tensor_list)\n                \n                # Process each result in this sub-batch\n                for j, output in enumerate(outputs):\n                    boxes = output['boxes']\n                    scores = output['scores']\n                    \n                    # Filter by confidence threshold\n                    keep_idxs = (scores >= CONFIDENCE_THRESHOLD).nonzero(as_tuple=False).view(-1)\n                    for idx in keep_idxs:\n                        confidence = float(scores[idx].cpu().item())\n                        x1, y1, x2, y2 = boxes[idx].cpu().numpy()\n                        x_center = (x1 + x2) / 2.0\n                        y_center = (y1 + y2) / 2.0\n                        \n                        all_detections.append({\n                            'z': sub_batch_slice_nums[j],\n                            'y': round(y_center),\n                            'x': round(x_center),\n                            'confidence': confidence\n                        })\n        \n        # Synchronize CUDA streams\n        if device.startswith('cuda'):\n            torch.cuda.synchronize()\n    \n    # Ensure final preload thread is joined\n    if next_batch_thread is not None:\n        next_batch_thread.join()\n    \n    # 3D Non-Maximum Suppression to merge close detections\n    final_detections = perform_3d_nms(all_detections, NMS_IOU_THRESHOLD)\n    final_detections.sort(key=lambda x: x['confidence'], reverse=True)\n    \n    # If no detections, return NA\n    if not final_detections:\n        return {\n            'tomo_id': tomo_id,\n            'Motor axis 0': -1,\n            'Motor axis 1': -1,\n            'Motor axis 2': -1\n        }\n    \n    best = final_detections[0]\n    return {\n        'tomo_id': tomo_id,\n        'Motor axis 0': round(best['z']),\n        'Motor axis 1': round(best['y']),\n        'Motor axis 2': round(best['x'])\n    }\n\ndef perform_3d_nms(detections, iou_threshold):\n    \"\"\"\n    Perform 3D Non-Maximum Suppression on detections to merge nearby motors.\n    \"\"\"\n    if not detections:\n        return []\n    \n    detections = sorted(detections, key=lambda x: x['confidence'], reverse=True)\n    final_dets = []\n    \n    def distance_3d(d1, d2):\n        return np.sqrt((d1['z'] - d2['z'])**2 +\n                       (d1['y'] - d2['y'])**2 +\n                       (d1['x'] - d2['x'])**2)\n    \n    box_size = 24\n    dist_thresh = box_size * iou_threshold\n    \n    while detections:\n        best = detections.pop(0)\n        final_dets.append(best)\n        detections = [d for d in detections if distance_3d(d, best) > dist_thresh]\n    \n    return final_dets\n\ndef debug_image_loading(tomo_id):\n    \"\"\"\n    Debug function: check image loading and Faster R-CNN inference on a sample slice.\n    \"\"\"\n    tomo_dir = os.path.join(test_dir, tomo_id)\n    slice_files = sorted([f for f in os.listdir(tomo_dir) if f.endswith('.jpg')])\n    if not slice_files:\n        print(f\"No image files found in {tomo_dir}\")\n        return\n    \n    print(f\"Found {len(slice_files)} image files in {tomo_dir}\")\n    sample_file = slice_files[len(slice_files)//2]\n    img_path = os.path.join(tomo_dir, sample_file)\n    \n    try:\n        img_pil = Image.open(img_path)\n        img_arr = np.array(img_pil)\n        print(f\"PIL load: shape {img_arr.shape}, dtype {img_arr.dtype}\")\n        \n        img_cv2 = cv2.imread(img_path, cv2.IMREAD_GRAYSCALE)\n        print(f\"OpenCV load (grayscale): shape {img_cv2.shape}, dtype {img_cv2.dtype}\")\n        \n        rgb = cv2.cvtColor(img_cv2, cv2.COLOR_GRAY2RGB)\n        print(f\"Converted to RGB: shape {rgb.shape}, dtype {rgb.dtype}\")\n        \n        print(\"Image loading successful!\")\n    except Exception as e:\n        print(f\"Error loading {img_path}: {e}\")\n    \n    # Test a single inference\n    try:\n        # Build the same model instantiation logic to load weights temporarily\n        num_classes = 2  # Adjust if your model was trained with a different number of classes\n        backbone = fasterrcnn_resnet50_fpn(pretrained=False)\n        in_features = backbone.roi_heads.box_predictor.cls_score.in_features\n        backbone.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n        backbone.load_state_dict(torch.load(model_path, map_location=device))\n        backbone.to(device).eval()\n        \n        # Prepare one image\n        raw = cv2.imread(img_path, cv2.IMREAD_GRAYSCALE)\n        normalized = normalize_slice(raw)\n        rgb = cv2.cvtColor(normalized, cv2.COLOR_GRAY2RGB)\n        pil_img = Image.fromarray(rgb)\n        img_tensor = T.ToTensor()(pil_img).to(device)\n        \n        with torch.no_grad():\n            output = backbone([img_tensor])\n        print(\"Faster R-CNN ran on a sample slice successfully!\")\n        print(f\"Output keys: {list(output[0].keys())}\")\n    except Exception as e:\n        print(f\"Error during Faster R-CNN inference: {e}\")\n\ndef generate_submission():\n    \"\"\"\n    Main function to generate the submission CSV.\n    \"\"\"\n    # List all tomogram folders\n    test_tomos = sorted([d for d in os.listdir(test_dir) if os.path.isdir(os.path.join(test_dir, d))])\n    total_tomos = len(test_tomos)\n    print(f\"Found {total_tomos} tomograms in test directory\")\n    \n    # Debug image loading & model on the first tomogram\n    if test_tomos:\n        debug_image_loading(test_tomos[0])\n    \n    # Clear CUDA cache before starting\n    if torch.cuda.is_available():\n        torch.cuda.empty_cache()\n    \n    # Initialize Faster R-CNN model once\n    print(f\"Loading Faster R-CNN model from {model_path}\")\n    num_classes = 2  # background + motor; adjust if necessary\n    model = fasterrcnn_resnet50_fpn(pretrained=False)\n    in_features = model.roi_heads.box_predictor.cls_score.in_features\n    model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n    state_dict = torch.load(model_path, map_location=device)\n    model.load_state_dict(state_dict)\n    model.to(device).eval()\n    \n    results = []\n    motors_found = 0\n    \n    # Use ThreadPoolExecutor to parallelize tomogram processing\n    with ThreadPoolExecutor(max_workers=1) as executor:\n        future_to_tomo = {}\n        for i, tomo_id in enumerate(test_tomos, 1):\n            future = executor.submit(process_tomogram, tomo_id, model, i, total_tomos)\n            future_to_tomo[future] = tomo_id\n        \n        for future in future_to_tomo:\n            tomo_id = future_to_tomo[future]\n            try:\n                if torch.cuda.is_available():\n                    torch.cuda.empty_cache()\n                \n                result = future.result()\n                results.append(result)\n                \n                # Count detected motors\n                if result['Motor axis 0'] != -1:\n                    motors_found += 1\n                    print(f\"Motor found in {tomo_id} at z={result['Motor axis 0']}, \"\n                          f\"y={result['Motor axis 1']}, x={result['Motor axis 2']}\")\n                else:\n                    print(f\"No motor detected in {tomo_id}\")\n                \n                pct = motors_found / len(results) * 100\n                print(f\"Current detection rate: {motors_found}/{len(results)} ({pct:.1f}%)\")\n            \n            except Exception as e:\n                print(f\"Error processing {tomo_id}: {e}\")\n                results.append({\n                    'tomo_id': tomo_id,\n                    'Motor axis 0': -1,\n                    'Motor axis 1': -1,\n                    'Motor axis 2': -1\n                })\n    \n    # Build submission DataFrame\n    submission_df = pd.DataFrame(results)\n    submission_df = submission_df[['tomo_id', 'Motor axis 0', 'Motor axis 1', 'Motor axis 2']]\n    submission_df.to_csv(submission_path, index=False)\n    \n    print(f\"\\nSubmission complete!\")\n    print(f\"Motors detected: {motors_found}/{total_tomos} ({motors_found/total_tomos*100:.1f}%)\")\n    print(f\"Submission saved to: {submission_path}\")\n    print(\"\\nSubmission preview:\")\n    print(submission_df.head())\n    \n    return submission_df\n\n# Run the pipeline\nif __name__ == \"__main__\":\n    start_time = time.time()\n    submission = generate_submission()\n    elapsed = time.time() - start_time\n    print(f\"\\nTotal execution time: {elapsed:.2f} seconds ({elapsed/60:.2f} minutes)\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-03T19:22:22.499099Z","iopub.execute_input":"2025-06-03T19:22:22.499377Z","iopub.status.idle":"2025-06-03T19:26:15.820730Z","shell.execute_reply.started":"2025-06-03T19:22:22.499357Z","shell.execute_reply":"2025-06-03T19:26:15.819528Z"}},"outputs":[{"name":"stdout","text":"Using GPU: Tesla T4 with 15.83 GB memory\nDynamic batch size set to 32 based on 15.66 GB free memory\nFound 3 tomograms in test directory\nFound 500 image files in /kaggle/input/byu-locating-bacterial-flagellar-motors-2025/test/tomo_003acc\nPIL load: shape (1912, 1847), dtype uint8\nOpenCV load (grayscale): shape (1912, 1847), dtype uint8\nConverted to RGB: shape (1912, 1847, 3), dtype uint8\nImage loading successful!\n","output_type":"stream"},{"name":"stderr","text":"<ipython-input-6-1332d5111f43>:260: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  backbone.load_state_dict(torch.load(model_path, map_location=device))\n","output_type":"stream"},{"name":"stdout","text":"Faster R-CNN ran on a sample slice successfully!\nOutput keys: ['boxes', 'labels', 'scores']\nLoading Faster R-CNN model from /kaggle/input/test-faster-rcnn/pytorch/default/1/fasterrcnn_motor_detector_2.pth\n","output_type":"stream"},{"name":"stderr","text":"<ipython-input-6-1332d5111f43>:300: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  state_dict = torch.load(model_path, map_location=device)\n","output_type":"stream"},{"name":"stdout","text":"Processing tomogram tomo_003acc (1/3)\nProcessing 500 out of 500 slices based on CONCENTRATION=1\n[PROFILE] Inference batch 1/4: 7.685s\n[PROFILE] Inference batch 2/4: 0.662s\n[PROFILE] Inference batch 3/4: 0.660s\n[PROFILE] Inference batch 4/4: 0.657s\n[PROFILE] Inference batch 1/4: 0.766s\n[PROFILE] Inference batch 2/4: 0.665s\n[PROFILE] Inference batch 3/4: 0.665s\n[PROFILE] Inference batch 4/4: 0.664s\n[PROFILE] Inference batch 1/4: 0.760s\n[PROFILE] Inference batch 2/4: 0.665s\n[PROFILE] Inference batch 3/4: 0.666s\n[PROFILE] Inference batch 4/4: 0.667s\n[PROFILE] Inference batch 1/4: 0.666s\n[PROFILE] Inference batch 2/4: 0.670s\n[PROFILE] Inference batch 3/4: 0.670s\n[PROFILE] Inference batch 4/4: 0.670s\n[PROFILE] Inference batch 1/4: 0.675s\n[PROFILE] Inference batch 2/4: 0.673s\n[PROFILE] Inference batch 3/4: 0.677s\n[PROFILE] Inference batch 4/4: 0.677s\n[PROFILE] Inference batch 1/4: 0.678s\n[PROFILE] Inference batch 2/4: 0.669s\n[PROFILE] Inference batch 3/4: 0.675s\n[PROFILE] Inference batch 4/4: 0.676s\n[PROFILE] Inference batch 1/4: 0.675s\n[PROFILE] Inference batch 2/4: 0.676s\n[PROFILE] Inference batch 3/4: 0.677s\n[PROFILE] Inference batch 4/4: 0.681s\n[PROFILE] Inference batch 1/4: 0.676s\n[PROFILE] Inference batch 2/4: 0.677s\n[PROFILE] Inference batch 3/4: 0.677s\n[PROFILE] Inference batch 4/4: 0.682s\n[PROFILE] Inference batch 1/4: 0.681s\n[PROFILE] Inference batch 2/4: 0.682s\n[PROFILE] Inference batch 3/4: 0.694s\n[PROFILE] Inference batch 4/4: 0.689s\n[PROFILE] Inference batch 1/4: 0.683s\n[PROFILE] Inference batch 2/4: 0.684s\n[PROFILE] Inference batch 3/4: 0.685s\n[PROFILE] Inference batch 4/4: 0.683s\n[PROFILE] Inference batch 1/4: 0.685s\n[PROFILE] Inference batch 2/4: 0.689s\n[PROFILE] Inference batch 3/4: 0.694s\n[PROFILE] Inference batch 4/4: 0.688s\n[PROFILE] Inference batch 1/4: 0.694s\n[PROFILE] Inference batch 2/4: 0.702s\n[PROFILE] Inference batch 3/4: 0.692s\n[PROFILE] Inference batch 4/4: 0.693s\n[PROFILE] Inference batch 1/4: 0.698s\n[PROFILE] Inference batch 2/4: 0.695s\n[PROFILE] Inference batch 3/4: 0.700s\n[PROFILE] Inference batch 4/4: 0.698s\n[PROFILE] Inference batch 1/4: 0.700s\n[PROFILE] Inference batch 2/4: 0.701s\n[PROFILE] Inference batch 3/4: 0.700s\n[PROFILE] Inference batch 4/4: 0.699s\n[PROFILE] Inference batch 1/4: 0.703s\n[PROFILE] Inference batch 2/4: 0.698s\n[PROFILE] Inference batch 3/4: 0.701s\n[PROFILE] Inference batch 4/4: 0.716s\n[PROFILE] Inference batch 1/4: 6.096s\n[PROFILE] Inference batch 2/4: 0.447s\n[PROFILE] Inference batch 3/4: 0.434s\n[PROFILE] Inference batch 4/4: 0.442s\nProcessing tomogram tomo_00e047 (2/3)\nNo motor detected in tomo_003acc\nCurrent detection rate: 0/1 (0.0%)\nProcessing 300 out of 300 slices based on CONCENTRATION=1\n[PROFILE] Inference batch 1/4: 0.705s\n[PROFILE] Inference batch 2/4: 0.701s\n[PROFILE] Inference batch 3/4: 0.702s\n[PROFILE] Inference batch 4/4: 0.700s\n[PROFILE] Inference batch 1/4: 0.699s\n[PROFILE] Inference batch 2/4: 0.694s\n[PROFILE] Inference batch 3/4: 0.695s\n[PROFILE] Inference batch 4/4: 0.808s\n[PROFILE] Inference batch 1/4: 0.699s\n[PROFILE] Inference batch 2/4: 0.699s\n[PROFILE] Inference batch 3/4: 0.703s\n[PROFILE] Inference batch 4/4: 0.696s\n[PROFILE] Inference batch 1/4: 0.694s\n[PROFILE] Inference batch 2/4: 0.692s\n[PROFILE] Inference batch 3/4: 0.697s\n[PROFILE] Inference batch 4/4: 0.697s\n[PROFILE] Inference batch 1/4: 0.698s\n[PROFILE] Inference batch 2/4: 0.695s\n[PROFILE] Inference batch 3/4: 0.694s\n[PROFILE] Inference batch 4/4: 0.701s\n[PROFILE] Inference batch 1/4: 0.699s\n[PROFILE] Inference batch 2/4: 0.697s\n[PROFILE] Inference batch 3/4: 0.693s\n[PROFILE] Inference batch 4/4: 0.687s\n[PROFILE] Inference batch 1/4: 0.700s\n[PROFILE] Inference batch 2/4: 0.699s\n[PROFILE] Inference batch 3/4: 0.689s\n[PROFILE] Inference batch 4/4: 0.700s\n[PROFILE] Inference batch 1/4: 0.690s\n[PROFILE] Inference batch 2/4: 0.688s\n[PROFILE] Inference batch 3/4: 0.693s\n[PROFILE] Inference batch 4/4: 0.685s\n[PROFILE] Inference batch 1/4: 0.691s\n[PROFILE] Inference batch 2/4: 0.688s\n[PROFILE] Inference batch 3/4: 0.688s\n[PROFILE] Inference batch 4/4: 0.687s\n[PROFILE] Inference batch 1/4: 4.636s\n[PROFILE] Inference batch 2/4: 0.259s\n[PROFILE] Inference batch 3/4: 0.258s\n[PROFILE] Inference batch 4/4: 0.260s\nProcessing tomogram tomo_01a877 (3/3)\nMotor found in tomo_00e047 at z=171, y=547, x=606\nCurrent detection rate: 1/2 (50.0%)\nProcessing 300 out of 300 slices based on CONCENTRATION=1\n[PROFILE] Inference batch 1/4: 0.691s\n[PROFILE] Inference batch 2/4: 0.692s\n[PROFILE] Inference batch 3/4: 0.686s\n[PROFILE] Inference batch 4/4: 0.686s\n[PROFILE] Inference batch 1/4: 0.698s\n[PROFILE] Inference batch 2/4: 0.691s\n[PROFILE] Inference batch 3/4: 0.683s\n[PROFILE] Inference batch 4/4: 0.798s\n[PROFILE] Inference batch 1/4: 0.694s\n[PROFILE] Inference batch 2/4: 0.698s\n[PROFILE] Inference batch 3/4: 0.688s\n[PROFILE] Inference batch 4/4: 0.693s\n[PROFILE] Inference batch 1/4: 0.687s\n[PROFILE] Inference batch 2/4: 0.690s\n[PROFILE] Inference batch 3/4: 0.690s\n[PROFILE] Inference batch 4/4: 0.687s\n[PROFILE] Inference batch 1/4: 0.687s\n[PROFILE] Inference batch 2/4: 0.697s\n[PROFILE] Inference batch 3/4: 0.697s\n[PROFILE] Inference batch 4/4: 0.693s\n[PROFILE] Inference batch 1/4: 0.693s\n[PROFILE] Inference batch 2/4: 0.695s\n[PROFILE] Inference batch 3/4: 0.692s\n[PROFILE] Inference batch 4/4: 0.696s\n[PROFILE] Inference batch 1/4: 0.713s\n[PROFILE] Inference batch 2/4: 0.704s\n[PROFILE] Inference batch 3/4: 0.697s\n[PROFILE] Inference batch 4/4: 0.696s\n[PROFILE] Inference batch 1/4: 0.697s\n[PROFILE] Inference batch 2/4: 0.698s\n[PROFILE] Inference batch 3/4: 0.697s\n[PROFILE] Inference batch 4/4: 0.695s\n[PROFILE] Inference batch 1/4: 0.702s\n[PROFILE] Inference batch 2/4: 0.699s\n[PROFILE] Inference batch 3/4: 0.692s\n[PROFILE] Inference batch 4/4: 0.704s\n[PROFILE] Inference batch 1/4: 0.260s\n[PROFILE] Inference batch 2/4: 0.259s\n[PROFILE] Inference batch 3/4: 0.260s\n[PROFILE] Inference batch 4/4: 0.260s\nNo motor detected in tomo_01a877\nCurrent detection rate: 1/3 (33.3%)\n\nSubmission complete!\nMotors detected: 1/3 (33.3%)\nSubmission saved to: /kaggle/working/submission.csv\n\nSubmission preview:\n       tomo_id  Motor axis 0  Motor axis 1  Motor axis 2\n0  tomo_003acc            -1            -1            -1\n1  tomo_00e047           171           547           606\n2  tomo_01a877            -1            -1            -1\n\nTotal execution time: 231.79 seconds (3.86 minutes)\n","output_type":"stream"}],"execution_count":6}]}