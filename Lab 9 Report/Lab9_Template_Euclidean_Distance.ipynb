{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JohnTichenor/Locating-Bacterial-Flagellar-Motors/blob/main/Lab9_Template.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ],
      "id": "view-in-github"
    },
    {
      "cell_type": "markdown",
      "id": "7f1be33f",
      "metadata": {
        "id": "7f1be33f"
      },
      "source": [
        "# Lab 9 Report:\n",
        "## Final Project Codebase"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "af7b72a9",
      "metadata": {
        "id": "af7b72a9"
      },
      "source": [
        "## Project Title: Detecting Bacterial Flagellar Motors Using Faster R-CNN"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bf02d733",
      "metadata": {
        "id": "bf02d733"
      },
      "source": [
        "### Group Members: John Tichenor Maddox Spinelli"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a935c2e8",
      "metadata": {
        "id": "a935c2e8"
      },
      "source": [
        "--------------------"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Install Packages"
      ],
      "metadata": {
        "id": "AjoBy7zBbuez"
      },
      "id": "AjoBy7zBbuez"
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install torch torchvision\n",
        "# !pip install tqdm"
      ],
      "metadata": {
        "id": "CinsH9hKbxCr"
      },
      "id": "CinsH9hKbxCr",
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Import Dependencies"
      ],
      "metadata": {
        "id": "1Pi6E1s-DmrT"
      },
      "id": "1Pi6E1s-DmrT"
    },
    {
      "cell_type": "code",
      "source": [
        "# ==== PyTorch Core ====\n",
        "import torch\n",
        "from torch.utils.data import DataLoader, Dataset, random_split\n",
        "\n",
        "# ==== TorchVision for Computer Vision ====\n",
        "import torchvision\n",
        "from torchvision.models.detection import fasterrcnn_resnet50_fpn  # Prebuilt Faster R-CNN model\n",
        "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor  # Custom predictor head\n",
        "import torchvision.transforms as T  # Standard (v1) transforms\n",
        "import torchvision.transforms.v2 as T  # v2 transforms for detection tasks (handle boxes/labels)\n",
        "from torchvision.datasets import ImageFolder  # For quick folder-based datasets\n",
        "\n",
        "# ==== Data Handling and Utilities ====\n",
        "from PIL import Image  # Image loading/manipulation\n",
        "import json  # JSON file reading/writing\n",
        "import os  # Operating system utilities\n",
        "import numpy as np  # Numerical computations\n",
        "import pandas as pd  # DataFrame support\n",
        "\n",
        "# ==== Progress Bars and Parallel Processing ====\n",
        "from tqdm.notebook import tqdm  # Jupyter-friendly progress bar\n",
        "import threading  # Threading for concurrent tasks\n",
        "from contextlib import nullcontext  # Context manager utility\n",
        "from concurrent.futures import ThreadPoolExecutor  # For thread pools\n",
        "import time  # Timing utilities\n",
        "\n",
        "# ==== Visualization ====\n",
        "import matplotlib.pyplot as plt  # Plotting\n",
        "\n",
        "# ==== Metrics ====\n",
        "from sklearn.metrics import fbeta_score  # F-beta metric for evaluation\n",
        "\n",
        "import cv2  # OpenCV for advanced image and video operations\n"
      ],
      "metadata": {
        "id": "bnOd8QQADlkO"
      },
      "id": "bnOd8QQADlkO",
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Mount Drive"
      ],
      "metadata": {
        "id": "BOVEOR1vDs1p"
      },
      "id": "BOVEOR1vDs1p"
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bpcGIZ9J4HQG",
        "outputId": "bbb25d1f-d93b-4d31-eeac-3047187254a0"
      },
      "id": "bpcGIZ9J4HQG",
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qRgdEN2UDwhq",
        "outputId": "7eb68c1f-cc66-4450-a4d1-694f3a1a0820"
      },
      "id": "qRgdEN2UDwhq",
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "855b4738",
      "metadata": {
        "id": "855b4738"
      },
      "source": [
        "## Load Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "id": "8382e6f7",
      "metadata": {
        "id": "8382e6f7"
      },
      "outputs": [],
      "source": [
        "#To be adjusted based on user\n",
        "TRAIN_DIR = \"/content/drive/MyDrive/BacterialFlagellarMotorsData/train\"\n",
        "TEST_DIR = \"/content/drive/MyDrive/BacterialFlagellarMotorsData/test\"\n",
        "LABELS_PATH = \"/content/drive/MyDrive/BacterialFlagellarMotorsData/train_labels.csv\"\n",
        "SUBMIT_PATH = \"/content/drive/MyDrive/BacterialFlagellarMotorsData/submission.csv\"\n",
        "BOUNDING_BOXES_PATH = \"/content/drive/MyDrive/BacterialFlagellarMotorsData/bounding_boxes_32.json\"\n",
        "\n",
        "#For testing a previously trained model\n",
        "\"\"\"\n",
        "MODEL_PATH = \"/content/drive/MyDrive/BacterialFlagellarMotorsData/SavedModels/exp_5/model.pth\"\n",
        "\"\"\"\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Format Dataset"
      ],
      "metadata": {
        "id": "Bc2tQhIScnzB"
      },
      "id": "Bc2tQhIScnzB"
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "sJNsjsUq_WUm"
      },
      "id": "sJNsjsUq_WUm",
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {
        "id": "7L7HtaXBQGsY"
      },
      "outputs": [],
      "source": [
        "labels_df = pd.read_csv(LABELS_PATH)\n",
        "\n",
        "class TomogramDataset(Dataset):\n",
        "    def __init__(self, root_dir, json_path, transforms=None):\n",
        "        \"\"\"\n",
        "        root_dir: path to the folder containing tomogram subfolders\n",
        "        json_path: bounding box annotations (from motor_bounding_boxes.json)\n",
        "        \"\"\"\n",
        "        self.root_dir = root_dir\n",
        "        self.transforms = transforms\n",
        "\n",
        "        with open(json_path) as f:\n",
        "            self.annotations = json.load(f)\n",
        "\n",
        "        self.image_paths = list(self.annotations.keys())  # e.g., tomo_00e047/slice_0169.jpg\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.image_paths)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        rel_path = self.image_paths[idx]\n",
        "        img_path = os.path.join(self.root_dir, rel_path)\n",
        "\n",
        "        image = Image.open(img_path).convert(\"RGB\")\n",
        "\n",
        "        boxes = torch.tensor([ann[\"bbox\"] for ann in self.annotations[rel_path]], dtype=torch.float32)\n",
        "        labels = torch.tensor([ann[\"label\"] for ann in self.annotations[rel_path]], dtype=torch.int64)\n",
        "\n",
        "        # Fix for empty boxes: always shape (N, 4), even if N==0\n",
        "        if boxes.numel() == 0:\n",
        "            boxes = boxes.reshape(0, 4)\n",
        "        if labels.numel() == 0:\n",
        "            labels = labels.reshape(0,)\n",
        "\n",
        "        target = {\n",
        "            \"boxes\": boxes,\n",
        "            \"labels\": labels,\n",
        "            \"image_id\": torch.tensor([idx]),\n",
        "            \"area\": (boxes[:, 2] - boxes[:, 0]) * (boxes[:, 3] - boxes[:, 1]),\n",
        "            \"iscrowd\": torch.zeros((len(boxes),), dtype=torch.int64)\n",
        "        }\n",
        "\n",
        "        if self.transforms:\n",
        "          image, target = self.transforms(image, target)\n",
        "\n",
        "        return image, target"
      ],
      "id": "7L7HtaXBQGsY"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Transform and Split Data"
      ],
      "metadata": {
        "id": "6_KYVJD7EUKT"
      },
      "id": "6_KYVJD7EUKT"
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {
        "id": "83cwmeHSVnrY"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import Subset\n",
        "\n",
        "# Define the transformation pipeline for training data\n",
        "train_transforms = T.Compose([\n",
        "    T.ToImage(),                                # Convert PIL image to torch.Tensor and add metadata\n",
        "    T.ToDtype(torch.float32, scale=True),       # Ensure tensor is float32 and scaled to [0,1]\n",
        "    #T.RandomHorizontalFlip(p=0.5),              # Randomly flip images and boxes horizontally (augmentation)\n",
        "    #T.RandomRotation(degrees=10),               # Randomly rotate images and boxes (augmentation)\n",
        "    T.ColorJitter(brightness=0.2, contrast=0.2),# Randomly change brightness and contrast (augmentation)\n",
        "    T.Normalize(mean=[0.4831], std=[0.2198]),   # Normalize using computed dataset mean and std\n",
        "])\n",
        "\n",
        "# Define the transformation pipeline for validation data (no augmentation!)\n",
        "val_transforms = T.Compose([\n",
        "    T.ToImage(),                                # Convert PIL image to torch.Tensor\n",
        "    T.ToDtype(torch.float32, scale=True),       # Convert to float32 and scale\n",
        "    T.Normalize(mean=[0.4831], std=[0.2198]),   # Use same normalization as training\n",
        "])\n",
        "\n",
        "# Instantiate the full dataset WITHOUT transforms,\n",
        "# just to get a stable, reproducible split of train/val indices\n",
        "full_dataset = TomogramDataset(\n",
        "    root_dir=TRAIN_DIR,\n",
        "    json_path=BOUNDING_BOXES_PATH,\n",
        "    transforms=None  # No transforms needed for just splitting!\n",
        ")\n",
        "\n",
        "# Calculate split sizes for 80/20 train/validation split\n",
        "num_total = len(full_dataset)\n",
        "num_train = int(0.8 * num_total)\n",
        "num_val = num_total - num_train\n",
        "\n",
        "# Randomly permute indices for reproducible splitting\n",
        "generator = torch.Generator().manual_seed(42)\n",
        "indices = torch.randperm(num_total, generator=generator).tolist()\n",
        "\n",
        "# Assign indices to training and validation sets\n",
        "train_indices = indices[:num_train]\n",
        "val_indices = indices[num_train:]\n",
        "\n",
        "# Create two dataset objects, each with their respective transforms\n",
        "train_full = TomogramDataset(\n",
        "    root_dir=TRAIN_DIR,\n",
        "    json_path=BOUNDING_BOXES_PATH,\n",
        "    transforms=train_transforms,   # With augmentations\n",
        ")\n",
        "\n",
        "val_full = TomogramDataset(\n",
        "    root_dir=TRAIN_DIR,\n",
        "    json_path=BOUNDING_BOXES_PATH,\n",
        "    transforms=val_transforms,     # No augmentations\n",
        ")\n",
        "\n",
        "# Create Subsets using the split indices (so each only sees its own portion of data)\n",
        "train_dataset = Subset(train_full, train_indices)\n",
        "val_dataset = Subset(val_full, val_indices)\n",
        "\n",
        "# Create DataLoaders for training and validation\n",
        "# The collate_fn lambda is needed for batching object detection targets\n",
        "train_data_loader = DataLoader(\n",
        "    train_dataset,\n",
        "    batch_size=4,\n",
        "    shuffle=True,\n",
        "    collate_fn=lambda x: tuple(zip(*x))\n",
        ")\n",
        "\n",
        "validation_data_loader = DataLoader(\n",
        "    val_dataset,\n",
        "    batch_size=4,\n",
        "    shuffle=False,\n",
        "    collate_fn=lambda x: tuple(zip(*x))\n",
        ")\n",
        "\n"
      ],
      "id": "83cwmeHSVnrY"
    },
    {
      "cell_type": "markdown",
      "id": "cfda0488",
      "metadata": {
        "id": "cfda0488"
      },
      "source": [
        "## Define Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "id": "55d5426b",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "55d5426b",
        "outputId": "af779c87-5b3e-44c9-a73d-1b084cc26612"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=FasterRCNN_ResNet50_FPN_Weights.COCO_V1`. You can also use `weights=FasterRCNN_ResNet50_FPN_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "FasterRCNN(\n",
              "  (transform): GeneralizedRCNNTransform(\n",
              "      Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
              "      Resize(min_size=(800,), max_size=1333, mode='bilinear')\n",
              "  )\n",
              "  (backbone): BackboneWithFPN(\n",
              "    (body): IntermediateLayerGetter(\n",
              "      (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
              "      (bn1): FrozenBatchNorm2d(64, eps=0.0)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
              "      (layer1): Sequential(\n",
              "        (0): Bottleneck(\n",
              "          (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn1): FrozenBatchNorm2d(64, eps=0.0)\n",
              "          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "          (bn2): FrozenBatchNorm2d(64, eps=0.0)\n",
              "          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn3): FrozenBatchNorm2d(256, eps=0.0)\n",
              "          (relu): ReLU(inplace=True)\n",
              "          (downsample): Sequential(\n",
              "            (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "            (1): FrozenBatchNorm2d(256, eps=0.0)\n",
              "          )\n",
              "        )\n",
              "        (1): Bottleneck(\n",
              "          (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn1): FrozenBatchNorm2d(64, eps=0.0)\n",
              "          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "          (bn2): FrozenBatchNorm2d(64, eps=0.0)\n",
              "          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn3): FrozenBatchNorm2d(256, eps=0.0)\n",
              "          (relu): ReLU(inplace=True)\n",
              "        )\n",
              "        (2): Bottleneck(\n",
              "          (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn1): FrozenBatchNorm2d(64, eps=0.0)\n",
              "          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "          (bn2): FrozenBatchNorm2d(64, eps=0.0)\n",
              "          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn3): FrozenBatchNorm2d(256, eps=0.0)\n",
              "          (relu): ReLU(inplace=True)\n",
              "        )\n",
              "      )\n",
              "      (layer2): Sequential(\n",
              "        (0): Bottleneck(\n",
              "          (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn1): FrozenBatchNorm2d(128, eps=0.0)\n",
              "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "          (bn2): FrozenBatchNorm2d(128, eps=0.0)\n",
              "          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn3): FrozenBatchNorm2d(512, eps=0.0)\n",
              "          (relu): ReLU(inplace=True)\n",
              "          (downsample): Sequential(\n",
              "            (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "            (1): FrozenBatchNorm2d(512, eps=0.0)\n",
              "          )\n",
              "        )\n",
              "        (1): Bottleneck(\n",
              "          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn1): FrozenBatchNorm2d(128, eps=0.0)\n",
              "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "          (bn2): FrozenBatchNorm2d(128, eps=0.0)\n",
              "          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn3): FrozenBatchNorm2d(512, eps=0.0)\n",
              "          (relu): ReLU(inplace=True)\n",
              "        )\n",
              "        (2): Bottleneck(\n",
              "          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn1): FrozenBatchNorm2d(128, eps=0.0)\n",
              "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "          (bn2): FrozenBatchNorm2d(128, eps=0.0)\n",
              "          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn3): FrozenBatchNorm2d(512, eps=0.0)\n",
              "          (relu): ReLU(inplace=True)\n",
              "        )\n",
              "        (3): Bottleneck(\n",
              "          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn1): FrozenBatchNorm2d(128, eps=0.0)\n",
              "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "          (bn2): FrozenBatchNorm2d(128, eps=0.0)\n",
              "          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn3): FrozenBatchNorm2d(512, eps=0.0)\n",
              "          (relu): ReLU(inplace=True)\n",
              "        )\n",
              "      )\n",
              "      (layer3): Sequential(\n",
              "        (0): Bottleneck(\n",
              "          (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn1): FrozenBatchNorm2d(256, eps=0.0)\n",
              "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "          (bn2): FrozenBatchNorm2d(256, eps=0.0)\n",
              "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn3): FrozenBatchNorm2d(1024, eps=0.0)\n",
              "          (relu): ReLU(inplace=True)\n",
              "          (downsample): Sequential(\n",
              "            (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "            (1): FrozenBatchNorm2d(1024, eps=0.0)\n",
              "          )\n",
              "        )\n",
              "        (1): Bottleneck(\n",
              "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn1): FrozenBatchNorm2d(256, eps=0.0)\n",
              "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "          (bn2): FrozenBatchNorm2d(256, eps=0.0)\n",
              "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn3): FrozenBatchNorm2d(1024, eps=0.0)\n",
              "          (relu): ReLU(inplace=True)\n",
              "        )\n",
              "        (2): Bottleneck(\n",
              "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn1): FrozenBatchNorm2d(256, eps=0.0)\n",
              "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "          (bn2): FrozenBatchNorm2d(256, eps=0.0)\n",
              "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn3): FrozenBatchNorm2d(1024, eps=0.0)\n",
              "          (relu): ReLU(inplace=True)\n",
              "        )\n",
              "        (3): Bottleneck(\n",
              "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn1): FrozenBatchNorm2d(256, eps=0.0)\n",
              "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "          (bn2): FrozenBatchNorm2d(256, eps=0.0)\n",
              "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn3): FrozenBatchNorm2d(1024, eps=0.0)\n",
              "          (relu): ReLU(inplace=True)\n",
              "        )\n",
              "        (4): Bottleneck(\n",
              "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn1): FrozenBatchNorm2d(256, eps=0.0)\n",
              "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "          (bn2): FrozenBatchNorm2d(256, eps=0.0)\n",
              "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn3): FrozenBatchNorm2d(1024, eps=0.0)\n",
              "          (relu): ReLU(inplace=True)\n",
              "        )\n",
              "        (5): Bottleneck(\n",
              "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn1): FrozenBatchNorm2d(256, eps=0.0)\n",
              "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "          (bn2): FrozenBatchNorm2d(256, eps=0.0)\n",
              "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn3): FrozenBatchNorm2d(1024, eps=0.0)\n",
              "          (relu): ReLU(inplace=True)\n",
              "        )\n",
              "      )\n",
              "      (layer4): Sequential(\n",
              "        (0): Bottleneck(\n",
              "          (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn1): FrozenBatchNorm2d(512, eps=0.0)\n",
              "          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "          (bn2): FrozenBatchNorm2d(512, eps=0.0)\n",
              "          (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn3): FrozenBatchNorm2d(2048, eps=0.0)\n",
              "          (relu): ReLU(inplace=True)\n",
              "          (downsample): Sequential(\n",
              "            (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "            (1): FrozenBatchNorm2d(2048, eps=0.0)\n",
              "          )\n",
              "        )\n",
              "        (1): Bottleneck(\n",
              "          (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn1): FrozenBatchNorm2d(512, eps=0.0)\n",
              "          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "          (bn2): FrozenBatchNorm2d(512, eps=0.0)\n",
              "          (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn3): FrozenBatchNorm2d(2048, eps=0.0)\n",
              "          (relu): ReLU(inplace=True)\n",
              "        )\n",
              "        (2): Bottleneck(\n",
              "          (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn1): FrozenBatchNorm2d(512, eps=0.0)\n",
              "          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "          (bn2): FrozenBatchNorm2d(512, eps=0.0)\n",
              "          (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn3): FrozenBatchNorm2d(2048, eps=0.0)\n",
              "          (relu): ReLU(inplace=True)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (fpn): FeaturePyramidNetwork(\n",
              "      (inner_blocks): ModuleList(\n",
              "        (0): Conv2dNormActivation(\n",
              "          (0): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
              "        )\n",
              "        (1): Conv2dNormActivation(\n",
              "          (0): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))\n",
              "        )\n",
              "        (2): Conv2dNormActivation(\n",
              "          (0): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))\n",
              "        )\n",
              "        (3): Conv2dNormActivation(\n",
              "          (0): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1))\n",
              "        )\n",
              "      )\n",
              "      (layer_blocks): ModuleList(\n",
              "        (0-3): 4 x Conv2dNormActivation(\n",
              "          (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "        )\n",
              "      )\n",
              "      (extra_blocks): LastLevelMaxPool()\n",
              "    )\n",
              "  )\n",
              "  (rpn): RegionProposalNetwork(\n",
              "    (anchor_generator): AnchorGenerator()\n",
              "    (head): RPNHead(\n",
              "      (conv): Sequential(\n",
              "        (0): Conv2dNormActivation(\n",
              "          (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "          (1): ReLU(inplace=True)\n",
              "        )\n",
              "      )\n",
              "      (cls_logits): Conv2d(256, 3, kernel_size=(1, 1), stride=(1, 1))\n",
              "      (bbox_pred): Conv2d(256, 12, kernel_size=(1, 1), stride=(1, 1))\n",
              "    )\n",
              "  )\n",
              "  (roi_heads): RoIHeads(\n",
              "    (box_roi_pool): MultiScaleRoIAlign(featmap_names=['0', '1', '2', '3'], output_size=(7, 7), sampling_ratio=2)\n",
              "    (box_head): TwoMLPHead(\n",
              "      (fc6): Linear(in_features=12544, out_features=1024, bias=True)\n",
              "      (fc7): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "    )\n",
              "    (box_predictor): FastRCNNPredictor(\n",
              "      (cls_score): Linear(in_features=1024, out_features=2, bias=True)\n",
              "      (bbox_pred): Linear(in_features=1024, out_features=8, bias=True)\n",
              "    )\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 63
        }
      ],
      "source": [
        "# Load a pre-trained Faster R-CNN\n",
        "model = fasterrcnn_resnet50_fpn(pretrained=True)\n",
        "\n",
        "# Replace the classification head\n",
        "NUM_CLASSES = 2  # 1 motor class + 1 background\n",
        "IN_FEATURES = model.roi_heads.box_predictor.cls_score.in_features\n",
        "model.roi_heads.box_predictor = FastRCNNPredictor(IN_FEATURES, NUM_CLASSES)\n",
        "\n",
        "#If loading previously trained model\n",
        "\"\"\"\n",
        "state_dict = torch.load(MODEL_PATH, map_location=device)\n",
        "model.load_state_dict(state_dict)\n",
        "\"\"\"\n",
        "\n",
        "# Put model on GPU\n",
        "model.to(device)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "aac20f78",
      "metadata": {
        "id": "aac20f78"
      },
      "source": [
        "## Define Hyperparameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "id": "4df0fd7d",
      "metadata": {
        "id": "4df0fd7d"
      },
      "outputs": [],
      "source": [
        "num_epochs = 15\n",
        "lr = 0.005\n",
        "score_threshold = 0.3\n",
        "scheduler_step = 3,\n",
        "scheduler_gamma = 0.3,"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ff8fba0e",
      "metadata": {
        "id": "ff8fba0e"
      },
      "source": [
        "## Identify Tracked Values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "id": "eccf5783",
      "metadata": {
        "id": "eccf5783"
      },
      "outputs": [],
      "source": [
        "train_losses = []\n",
        "val_f2s_all = []"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c403ae44",
      "metadata": {
        "id": "c403ae44"
      },
      "source": [
        "# Train Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 67,
      "metadata": {
        "id": "1iNK_IhKtfud",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 405,
          "referenced_widgets": [
            "7c98df1187f341c1a4dd3c453147e336",
            "6e83bfc0c7774676962f461ddd2bb4e2",
            "1f26f280a0eb415ea4a8031ab4887d94",
            "129eed4c28fa440aac01f13c83ea6d8b",
            "1ee8f952fd4d498fbeb8b0668639c7e1",
            "13868ee44a1545b8b20dc28f65cff35c",
            "7503231ee8b8474a9832dc315ba369a6",
            "b6a45115d77e46589c1f89378dd8fd38",
            "f3cc337af8334c0b8e31dad6b0259512",
            "0e9955a1dc984e5ab32acfadf99b6990",
            "16f086e88fff4c2d9d0e42fc0bc7446d"
          ]
        },
        "outputId": "432987fd-fe16-49ee-cac1-79175779e155"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Epoch 1/15:   0%|          | 0/147 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "7c98df1187f341c1a4dd3c453147e336"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-67-e89b1d00dc78>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    164\u001b[0m \u001b[0;31m# --------- Usage ---------\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    165\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 166\u001b[0;31m train_losses, val_f2s_all = train_and_validate_detector(\n\u001b[0m\u001b[1;32m    167\u001b[0m      \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_data_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_data_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcompute_f2_score\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    168\u001b[0m      \u001b[0mnum_epochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_epochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscore_threshold\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mscore_threshold\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscheduler_step\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mscheduler_step\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-67-e89b1d00dc78>\u001b[0m in \u001b[0;36mtrain_and_validate_detector\u001b[0;34m(model, train_data_loader, validation_data_loader, device, compute_f2_score, num_epochs, lr, score_threshold, scheduler_step, scheduler_gamma, train_losses, val_f2s_all)\u001b[0m\n\u001b[1;32m     98\u001b[0m         \u001b[0mpbar\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_data_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdesc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34mf\"Epoch {epoch + 1}/{num_epochs}\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mleave\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 100\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mimages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpbar\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    101\u001b[0m             \u001b[0;31m# Move images and targets to the selected device\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m             \u001b[0mimages\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mimg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mimages\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tqdm/notebook.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    248\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    249\u001b[0m             \u001b[0mit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__iter__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 250\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mobj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mit\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    251\u001b[0m                 \u001b[0;31m# return super(tqdm...) will not catch exception\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    252\u001b[0m                 \u001b[0;32myield\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tqdm/std.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1179\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1180\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1181\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mobj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1182\u001b[0m                 \u001b[0;32myield\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1183\u001b[0m                 \u001b[0;31m# Update and possibly print the progressbar.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    706\u001b[0m                 \u001b[0;31m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    707\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 708\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    709\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    710\u001b[0m             if (\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    762\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    763\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 764\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    765\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    766\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory_device\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     48\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"__getitems__\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getitems__\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 50\u001b[0;31m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getitems__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     51\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataset.py\u001b[0m in \u001b[0;36m__getitems__\u001b[0;34m(self, indices)\u001b[0m\n\u001b[1;32m    418\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getitems__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindices\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mindices\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[attr-defined]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    419\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 420\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindices\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mindices\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    421\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    422\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataset.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    418\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getitems__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindices\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mindices\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[attr-defined]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    419\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 420\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindices\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mindices\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    421\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    422\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-61-9793a29da390>\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0mimg_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mroot_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrel_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m         \u001b[0mimage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"RGB\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0mboxes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mann\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"bbox\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mann\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mannotations\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mrel_path\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/PIL/Image.py\u001b[0m in \u001b[0;36mopen\u001b[0;34m(fp, mode, formats)\u001b[0m\n\u001b[1;32m   3514\u001b[0m         \u001b[0mexclusive_fp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3515\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3516\u001b[0;31m     \u001b[0mprefix\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m16\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3517\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3518\u001b[0m     \u001b[0mpreinit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "def compute_euclidean_distance(boxA, boxB):\n",
        "  \"\"\"\n",
        "  Compute the Euclidean distance between the centers of two bounding boxes.\n",
        "\n",
        "  Args:\n",
        "      boxA (array-like): [x1, y1, x2, y2]\n",
        "      boxB (array-like): [x1, y1, x2, y2]\n",
        "\n",
        "  Returns:\n",
        "      float: Euclidean distance between the centers\n",
        "  \"\"\"\n",
        "  coordA = np.array([(boxA[0] + boxA[2]) / 2, (boxA[1] + boxA[3]) / 2])\n",
        "  coordB = np.array([(boxB[0] + boxB[2]) / 2, (boxB[1] + boxB[3]) / 2])\n",
        "\n",
        "  return np.linalg.norm(coordA - coordB)\n",
        "\n",
        "\n",
        "def compute_f2_score(pred_boxes, gt_boxes, voxel_spacing, euclidean_threshold=1000):\n",
        "    \"\"\"\n",
        "    Compute F2 score for a single image, given predicted and ground-truth boxes.\n",
        "\n",
        "    Args:\n",
        "        pred_boxes (np.ndarray): Array of predicted boxes [N_pred, 4]\n",
        "        gt_boxes (np.ndarray): Array of ground truth boxes [N_gt, 4]\n",
        "        euclidean_threshold (float): Distance threshold to consider a detection a true positive.\n",
        "\n",
        "    Returns:\n",
        "        float: F2 score for this image\n",
        "    \"\"\"\n",
        "    matched_gt = set()\n",
        "    tp = 0  # True positives\n",
        "\n",
        "    # Match predicted boxes to ground-truth boxes\n",
        "    for pred_box in pred_boxes:\n",
        "        for i, gt_box in enumerate(gt_boxes):\n",
        "            if i in matched_gt:\n",
        "                continue  # Skip already matched gt boxes\n",
        "            if compute_euclidean_distance(pred_box, gt_box) <= (euclidean_threshold/voxel_spacing):\n",
        "                tp += 1\n",
        "                matched_gt.add(i)\n",
        "                break  # Move to next predicted box\n",
        "\n",
        "    fp = len(pred_boxes) - tp  # False positives\n",
        "    fn = len(gt_boxes) - tp    # False negatives\n",
        "\n",
        "    beta2 = 4  # beta^2 for F2 score (beta=2)\n",
        "\n",
        "    # Calculate precision and recall, avoid division by zero\n",
        "    precision = tp / (tp + fp) if (tp + fp) > 0 else 0.0\n",
        "    recall = tp / (tp + fn) if (tp + fn) > 0 else 0.0\n",
        "    if precision + recall == 0:\n",
        "        return 0.0\n",
        "\n",
        "    # F2 score formula\n",
        "    f2 = (1 + beta2) * (precision * recall) / (beta2 * precision + recall)\n",
        "    return f2\n",
        "\n",
        "\n",
        "def train_and_validate_detector(\n",
        "    model,\n",
        "    train_data_loader,\n",
        "    validation_data_loader,\n",
        "    device,\n",
        "    compute_f2_score,  # pass your F2 function\n",
        "    num_epochs=10,\n",
        "    lr=0.005,\n",
        "    score_threshold=0.1,\n",
        "    scheduler_step = 3,\n",
        "    scheduler_gamma = 0.5,\n",
        "    train_losses=None,\n",
        "    val_f2s_all=None,\n",
        "):\n",
        "    \"\"\"\n",
        "    Train and validate a PyTorch object detector with F2 evaluation.\n",
        "\n",
        "    Args:\n",
        "        model: PyTorch detection model (e.g., Faster R-CNN)\n",
        "        train_data_loader: Dataloader for training set\n",
        "        validation_data_loader: Dataloader for validation set\n",
        "        device: 'cuda' or 'cpu'\n",
        "        compute_f2_score: Function to compute F2 (pred_boxes, gt_boxes)\n",
        "        num_epochs: Number of training epochs\n",
        "        lr: Learning rate\n",
        "        score_threshold: Minimum score to keep predicted boxes\n",
        "        train_losses (list): List to store training losses\n",
        "        val_f2s_all (list): List to store validation F2 scores\n",
        "    \"\"\"\n",
        "    # Set up optimizer and scheduler\n",
        "    optimizer = torch.optim.SGD(\n",
        "        model.parameters(), lr=lr, momentum=0.9, weight_decay=0.0005\n",
        "    )\n",
        "    scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=3, gamma=0.5)\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        # --- Training phase ---\n",
        "        model.train()\n",
        "        total_loss = 0.0\n",
        "        pbar = tqdm(train_data_loader, desc=f\"Epoch {epoch + 1}/{num_epochs}\", leave=False)\n",
        "\n",
        "        for images, targets in pbar:\n",
        "            # Move images and targets to the selected device\n",
        "            images = [img.to(device) for img in images]\n",
        "            targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
        "\n",
        "\n",
        "            # Forward pass, compute loss\n",
        "            loss_dict = model(images, targets)\n",
        "            losses = sum(loss for loss in loss_dict.values())\n",
        "\n",
        "            # Backpropagation\n",
        "            optimizer.zero_grad()\n",
        "            losses.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            total_loss += losses.item()\n",
        "\n",
        "        # Average training loss for this epoch\n",
        "        avg_train_loss = total_loss / len(train_data_loader)\n",
        "        train_losses.append(avg_train_loss)\n",
        "        print(f\"[Epoch {epoch + 1}/{num_epochs}] Loss: {avg_train_loss:.4f}\")\n",
        "\n",
        "        # Step the learning rate scheduler\n",
        "        scheduler.step()\n",
        "\n",
        "        # --- Validation phase ---\n",
        "        model.eval()\n",
        "        val_f2s = []\n",
        "        with torch.no_grad():\n",
        "            for images, targets in tqdm(validation_data_loader, desc=\"Validation\", leave=False):\n",
        "                images = [img.to(device) for img in images]\n",
        "                gt_boxes_batch = [t[\"boxes\"].cpu().numpy() for t in targets]  # Get GT boxes as numpy arrays\n",
        "\n",
        "                # Get voxel_spacing_list for the current batch\n",
        "                voxel_spacing_list = []\n",
        "                for t in targets:\n",
        "                    dataset_idx = t[\"image_id\"].item()\n",
        "                    rel_path = full_dataset.image_paths[dataset_idx]\n",
        "                    tomo_id = rel_path.split(\"/\")[0]\n",
        "                    spacing = labels_df.loc[labels_df['tomo_id'] == tomo_id, 'Voxel spacing'].values\n",
        "                    if len(spacing) > 0:\n",
        "                        voxel_spacing_list.append(spacing[0])\n",
        "                    else:\n",
        "                        voxel_spacing_list.append(None)\n",
        "\n",
        "                outputs = model(images)  # Get model predictions\n",
        "\n",
        "                for pred, gt_boxes, voxel_spacing in zip(outputs, gt_boxes_batch, voxel_spacing_list):\n",
        "                    pred_boxes = pred[\"boxes\"].cpu().numpy()\n",
        "                    # Only keep predictions with score above threshold\n",
        "                    if \"scores\" in pred:\n",
        "                        keep = pred[\"scores\"].cpu().numpy() > score_threshold\n",
        "                        pred_boxes = pred_boxes[keep]\n",
        "                    # Compute F2 score for this sample\n",
        "                    f2 = compute_f2_score(pred_boxes, gt_boxes, voxel_spacing)\n",
        "                    val_f2s.append(f2)\n",
        "\n",
        "        # Average F2 score for this validation epoch\n",
        "        avg_f2 = sum(val_f2s) / len(val_f2s) if val_f2s else 0.0\n",
        "        val_f2s_all.append(avg_f2)\n",
        "        print(f\"[Epoch {epoch + 1}/{num_epochs}] Validation F2: {avg_f2:.4f}\")\n",
        "\n",
        "    return train_losses, val_f2s_all\n",
        "\n",
        "# --------- Usage ---------\n",
        "\n",
        "train_losses, val_f2s_all = train_and_validate_detector(\n",
        "     model, train_data_loader, validation_data_loader, device, compute_f2_score,\n",
        "     num_epochs=num_epochs, lr=lr, score_threshold=score_threshold, scheduler_step=scheduler_step,\n",
        "     scheduler_gamma=scheduler_gamma, train_losses=train_losses, val_f2s_all=val_f2s_all\n",
        ")"
      ],
      "id": "1iNK_IhKtfud"
    },
    {
      "cell_type": "markdown",
      "id": "e1b49cc5",
      "metadata": {
        "id": "e1b49cc5"
      },
      "source": [
        "## Visualize & Evaluate Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1d4440cd",
      "metadata": {
        "id": "1d4440cd"
      },
      "outputs": [],
      "source": [
        "def plot_loss_and_val(train_losses, val_f2s=None, val_losses=None, ylabel_loss=\"Loss\"):\n",
        "    \"\"\"\n",
        "    Plots training loss, and optionally validation F2 or validation loss curves.\n",
        "\n",
        "    Args:\n",
        "        train_losses (list): List of training loss values per epoch.\n",
        "        val_f2s (list, optional): List of validation F2 values per epoch.\n",
        "        val_losses (list, optional): List of validation loss values per epoch.\n",
        "        ylabel_loss (str): Y-axis label for the loss metric.\n",
        "    \"\"\"\n",
        "    # Create a range for the number of epochs (x-axis)\n",
        "    epochs = range(1, len(train_losses)+1)\n",
        "\n",
        "    # Start a new figure with appropriate size\n",
        "    plt.figure(figsize=(7,5))\n",
        "\n",
        "    # Plot training loss curve\n",
        "    plt.plot(epochs, train_losses, label=\"Train \" + ylabel_loss)\n",
        "\n",
        "    # Plot validation F2 curve if provided\n",
        "    if val_f2s is not None:\n",
        "        plt.plot(epochs, val_f2s, label=\"Validation F2\")\n",
        "\n",
        "    # Plot validation loss curve if provided\n",
        "    if val_losses is not None:\n",
        "        plt.plot(epochs, val_losses, label=\"Validation \" + ylabel_loss)\n",
        "\n",
        "    # Set axis labels and title\n",
        "    plt.xlabel(\"Epoch\")\n",
        "    plt.ylabel(ylabel_loss)\n",
        "    plt.title(\"Training Curve\")\n",
        "\n",
        "    # Show legend for the different curves\n",
        "    plt.legend()\n",
        "\n",
        "    # Add a grid for easier visual interpretation\n",
        "    plt.grid(True)\n",
        "\n",
        "    # Adjust layout to prevent label cutoff\n",
        "    plt.tight_layout()\n",
        "\n",
        "    # Show the plot\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "# Example usage:\n",
        "plot_loss_and_val(train_losses, val_f2s=val_f2s_all, ylabel_loss=\"F2\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Run Inference on the model and generate submission for kaggle competition (Code Provided by BYU Kaggle)"
      ],
      "metadata": {
        "id": "TeVnKd34HVAt"
      },
      "id": "TeVnKd34HVAt"
    },
    {
      "cell_type": "code",
      "source": [
        "# Set random seed for reproducibility\n",
        "np.random.seed(42)\n",
        "torch.manual_seed(42)\n",
        "\n",
        "# Detection parameters\n",
        "CONFIDENCE_THRESHOLD = 0.45  # Lower threshold to catch more potential motors\n",
        "MAX_DETECTIONS_PER_TOMO = 3  # Keep track of top N detections per tomogram\n",
        "NMS_IOU_THRESHOLD = 0.2  # Non-maximum suppression threshold for 3D clustering\n",
        "CONCENTRATION = 1 # ONLY PROCESS 1/20 slices for fast submission\n",
        "\n",
        "# GPU profiling context manager\n",
        "class GPUProfiler:\n",
        "    def __init__(self, name):\n",
        "        self.name = name\n",
        "        self.start_time = None\n",
        "\n",
        "    def __enter__(self):\n",
        "        if torch.cuda.is_available():\n",
        "            torch.cuda.synchronize()\n",
        "        self.start_time = time.time()\n",
        "        return self\n",
        "\n",
        "    def __exit__(self, *args):\n",
        "        if torch.cuda.is_available():\n",
        "          elapsed = time.time() - self.start_time\n",
        "          print(f\"[PROFILE] {self.name}: {elapsed:.3f}s\")\n",
        "\n",
        "# Check GPU availability and set up optimizations\n",
        "device = 'cuda:0' if torch.cuda.is_available() else 'cpu'\n",
        "BATCH_SIZE = 8  # Default batch size, will be adjusted dynamically if GPU available\n",
        "\n",
        "if device.startswith('cuda'):\n",
        "    # Set CUDA optimization flags\n",
        "    torch.backends.cudnn.benchmark = True\n",
        "    torch.backends.cudnn.deterministic = False\n",
        "    torch.backends.cuda.matmul.allow_tf32 = True  # Allow TF32 on Ampere GPUs\n",
        "    torch.backends.cudnn.allow_tf32 = True\n",
        "\n",
        "    # Print GPU info\n",
        "    gpu_name = torch.cuda.get_device_name(0)\n",
        "    gpu_mem = torch.cuda.get_device_properties(0).total_memory / 1e9  # Convert to GB\n",
        "    print(f\"Using GPU: {gpu_name} with {gpu_mem:.2f} GB memory\")\n",
        "\n",
        "    # Get available GPU memory and set batch size accordingly\n",
        "    free_mem = gpu_mem - torch.cuda.memory_allocated(0) / 1e9\n",
        "    BATCH_SIZE = max(8, min(32, int(free_mem * 4)))  # 4 images per GB as rough estimate\n",
        "    print(f\"Dynamic batch size set to {BATCH_SIZE} based on {free_mem:.2f}GB free memory\")\n",
        "else:\n",
        "    print(\"GPU not available, using CPU\")\n",
        "    BATCH_SIZE = 4  # Reduce batch size for CPU\n",
        "\n",
        "\n",
        "def load_and_preprocess_image(img_path):\n",
        "    # Loads image as PIL, converts to tensor, normalizes as expected by torchvision models\n",
        "    img = Image.open(img_path).convert(\"RGB\")\n",
        "    preprocess = T.Compose([\n",
        "        T.ToTensor(),\n",
        "        T.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "    ])\n",
        "    return preprocess(img)\n",
        "\n",
        "\n",
        "def preload_image_batch(file_paths):\n",
        "    \"\"\"Preload a batch of images to CPU memory\"\"\"\n",
        "    images = []\n",
        "    for path in file_paths:\n",
        "        img = cv2.imread(path)\n",
        "        if img is None:\n",
        "            # Try with PIL as fallback\n",
        "            img = np.array(Image.open(path))\n",
        "        images.append(img)\n",
        "    return images\n",
        "\n",
        "def process_tomogram(tomo_id, model, index=0, total=1):\n",
        "    print(f\"Processing tomogram {tomo_id} ({index}/{total})\")\n",
        "    tomo_dir = os.path.join(TEST_DIR, tomo_id)\n",
        "    slice_files = sorted([f for f in os.listdir(tomo_dir) if f.endswith('.jpg')])\n",
        "\n",
        "    selected_indices = np.linspace(0, len(slice_files)-1, int(len(slice_files) * CONCENTRATION))\n",
        "    selected_indices = np.round(selected_indices).astype(int)\n",
        "    slice_files = [slice_files[i] for i in selected_indices]\n",
        "\n",
        "    print(f\"Processing {len(slice_files)} out of {len(os.listdir(tomo_dir))} slices based on CONCENTRATION={CONCENTRATION}\")\n",
        "\n",
        "    all_detections = []\n",
        "\n",
        "    for idx, slice_file in enumerate(slice_files):\n",
        "        img_path = os.path.join(tomo_dir, slice_file)\n",
        "        slice_num = int(slice_file.split('_')[1].split('.')[0])\n",
        "\n",
        "        # Preprocess for Faster R-CNN\n",
        "        image_tensor = load_and_preprocess_image(img_path).to(device).unsqueeze(0)  # shape (1, C, H, W)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            outputs = model(image_tensor)\n",
        "\n",
        "        boxes = outputs[0]['boxes'].cpu().numpy()  # (N, 4)\n",
        "        scores = outputs[0]['scores'].cpu().numpy()  # (N,)\n",
        "        labels = outputs[0]['labels'].cpu().numpy()  # (N,)\n",
        "\n",
        "        # Filter out detections below confidence threshold and (optionally) by class\n",
        "        for box, score, label in zip(boxes, scores, labels):\n",
        "            if score >= CONFIDENCE_THRESHOLD:\n",
        "                x1, y1, x2, y2 = box\n",
        "                x_center = (x1 + x2) / 2\n",
        "                y_center = (y1 + y2) / 2\n",
        "\n",
        "                all_detections.append({\n",
        "                    'z': round(slice_num),\n",
        "                    'y': round(y_center),\n",
        "                    'x': round(x_center),\n",
        "                    'confidence': float(score)\n",
        "                })\n",
        "\n",
        "    # Non-maximum suppression as before\n",
        "    final_detections = perform_3d_nms(all_detections, NMS_IOU_THRESHOLD)\n",
        "    final_detections.sort(key=lambda x: x['confidence'], reverse=True)\n",
        "\n",
        "    if not final_detections:\n",
        "        return {\n",
        "            'tomo_id': tomo_id,\n",
        "            'Motor axis 0': -1,\n",
        "            'Motor axis 1': -1,\n",
        "            'Motor axis 2': -1\n",
        "        }\n",
        "\n",
        "    best_detection = final_detections[0]\n",
        "    return {\n",
        "        'tomo_id': tomo_id,\n",
        "        'Motor axis 0': round(best_detection['z']),\n",
        "        'Motor axis 1': round(best_detection['y']),\n",
        "        'Motor axis 2': round(best_detection['x'])\n",
        "    }\n",
        "\n",
        "\n",
        "def perform_3d_nms(detections, iou_threshold):\n",
        "    \"\"\"\n",
        "    Perform 3D Non-Maximum Suppression on detections to merge nearby motors\n",
        "    \"\"\"\n",
        "    if not detections:\n",
        "        return []\n",
        "\n",
        "    # Sort by confidence (highest first)\n",
        "    detections = sorted(detections, key=lambda x: x['confidence'], reverse=True)\n",
        "\n",
        "    # List to store final detections after NMS\n",
        "    final_detections = []\n",
        "\n",
        "    # Define 3D distance function\n",
        "    def distance_3d(d1, d2):\n",
        "        return np.sqrt((d1['z'] - d2['z'])**2 +\n",
        "                       (d1['y'] - d2['y'])**2 +\n",
        "                       (d1['x'] - d2['x'])**2)\n",
        "\n",
        "    # Maximum distance threshold (based on box size and slice gap)\n",
        "    box_size = 24  # Same as annotation box size\n",
        "    distance_threshold = box_size * iou_threshold\n",
        "\n",
        "    # Process each detection\n",
        "    while detections:\n",
        "        # Take the detection with highest confidence\n",
        "        best_detection = detections.pop(0)\n",
        "        final_detections.append(best_detection)\n",
        "\n",
        "        # Filter out detections that are too close to the best detection\n",
        "        detections = [d for d in detections if distance_3d(d, best_detection) > distance_threshold]\n",
        "\n",
        "    return final_detections\n",
        "\n",
        "def debug_image_loading(tomo_id):\n",
        "    \"\"\"\n",
        "    Debug function to check image loading\n",
        "    \"\"\"\n",
        "    tomo_dir = os.path.join(TEST_DIR, tomo_id)\n",
        "    slice_files = sorted([f for f in os.listdir(tomo_dir) if f.endswith('.jpg')])\n",
        "\n",
        "    if not slice_files:\n",
        "        print(f\"No image files found in {tomo_dir}\")\n",
        "        return\n",
        "\n",
        "    print(f\"Found {len(slice_files)} image files in {tomo_dir}\")\n",
        "    sample_file = slice_files[len(slice_files)//2]  # Middle slice\n",
        "    img_path = os.path.join(tomo_dir, sample_file)\n",
        "\n",
        "    # Try different loading methods\n",
        "    try:\n",
        "        # Method 1: PIL\n",
        "        img_pil = Image.open(img_path)\n",
        "        img_array_pil = np.array(img_pil)\n",
        "        print(f\"PIL Image shape: {img_array_pil.shape}, dtype: {img_array_pil.dtype}\")\n",
        "\n",
        "        # Method 2: OpenCV\n",
        "        img_cv2 = cv2.imread(img_path, cv2.IMREAD_GRAYSCALE)\n",
        "        print(f\"OpenCV Image shape: {img_cv2.shape}, dtype: {img_cv2.dtype}\")\n",
        "\n",
        "        # Method 3: Convert to RGB\n",
        "        img_rgb = cv2.cvtColor(cv2.imread(img_path), cv2.COLOR_BGR2RGB)\n",
        "        print(f\"OpenCV RGB Image shape: {img_rgb.shape}, dtype: {img_rgb.dtype}\")\n",
        "\n",
        "        print(\"Image loading successful!\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error loading image {img_path}: {e}\")\n",
        "\n",
        "\n",
        "def generate_submission():\n",
        "    \"\"\"\n",
        "    Main function to generate the submission file\n",
        "    \"\"\"\n",
        "    # Get list of test tomograms\n",
        "    test_tomos = sorted([d for d in os.listdir(TEST_DIR) if os.path.isdir(os.path.join(TEST_DIR, d))])\n",
        "    total_tomos = len(test_tomos)\n",
        "\n",
        "    print(f\"Found {total_tomos} tomograms in test directory\")\n",
        "\n",
        "    # Debug image loading for the first tomogram\n",
        "    if test_tomos:\n",
        "        debug_image_loading(test_tomos[0])\n",
        "\n",
        "    # Clear GPU cache before starting\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.empty_cache()\n",
        "\n",
        "    model.to(device)\n",
        "    model.eval()\n",
        "    print(f\"Loaded Faster R-CNN model onto {device}\")\n",
        "\n",
        "    # Process tomograms with parallelization\n",
        "    results = []\n",
        "    motors_found = 0\n",
        "\n",
        "    # Using ThreadPoolExecutor with max_workers=1 since each worker uses the GPU already\n",
        "    # and we're parallelizing within each tomogram processing\n",
        "    with ThreadPoolExecutor(max_workers=1) as executor:\n",
        "        future_to_tomo = {}\n",
        "\n",
        "        # Submit all tomograms for processing\n",
        "        for i, tomo_id in enumerate(test_tomos, 1):\n",
        "            future = executor.submit(process_tomogram, tomo_id, model, i, total_tomos)\n",
        "            future_to_tomo[future] = tomo_id\n",
        "\n",
        "        # Process completed futures as they complete\n",
        "        for future in future_to_tomo:\n",
        "            tomo_id = future_to_tomo[future]\n",
        "            try:\n",
        "                # Clear CUDA cache between tomograms\n",
        "                if torch.cuda.is_available():\n",
        "                    torch.cuda.empty_cache()\n",
        "\n",
        "                result = future.result()\n",
        "                results.append(result)\n",
        "\n",
        "                # Update motors found count\n",
        "                has_motor = not pd.isna(result['Motor axis 0'])\n",
        "                if has_motor:\n",
        "                    motors_found += 1\n",
        "                    print(f\"Motor found in {tomo_id} at position: \"\n",
        "                          f\"z={result['Motor axis 0']}, y={result['Motor axis 1']}, x={result['Motor axis 2']}\")\n",
        "                else:\n",
        "                    print(f\"No motor detected in {tomo_id}\")\n",
        "\n",
        "                print(f\"Current detection rate: {motors_found}/{len(results)} ({motors_found/len(results)*100:.1f}%)\")\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"Error processing {tomo_id}: {e}\")\n",
        "                # Create a default entry for failed tomograms\n",
        "                results.append({\n",
        "                    'tomo_id': tomo_id,\n",
        "                    'Motor axis 0': -1,\n",
        "                    'Motor axis 1': -1,\n",
        "                    'Motor axis 2': -1\n",
        "                })\n",
        "\n",
        "    # Create submission dataframe\n",
        "    submission_df = pd.DataFrame(results)\n",
        "\n",
        "    # Ensure proper column order\n",
        "    submission_df = submission_df[['tomo_id', 'Motor axis 0', 'Motor axis 1', 'Motor axis 2']]\n",
        "\n",
        "    # Save the submission file\n",
        "    submission_df.to_csv(SUBMIT_PATH, index=False)\n",
        "\n",
        "    print(f\"\\nSubmission complete!\")\n",
        "    print(f\"Motors detected: {motors_found}/{total_tomos} ({motors_found/total_tomos*100:.1f}%)\")\n",
        "    print(f\"Submission saved to: {SUBMIT_PATH}\")\n",
        "\n",
        "    # Display first few rows of submission\n",
        "    print(\"\\nSubmission preview:\")\n",
        "    print(submission_df.head())\n",
        "\n",
        "    return submission_df\n",
        "\n",
        "# Run the submission pipeline\n",
        "if __name__ == \"__main__\":\n",
        "    # Time entire process\n",
        "    start_time = time.time()\n",
        "\n",
        "    # Generate submission\n",
        "    submission = generate_submission()\n",
        "\n",
        "    # Print total execution time\n",
        "    elapsed = time.time() - start_time\n",
        "    print(f\"\\nTotal execution time: {elapsed:.2f} seconds ({elapsed/60:.2f} minutes)\")"
      ],
      "metadata": {
        "id": "muEQ0oYsHXrd"
      },
      "id": "muEQ0oYsHXrd",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    },
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "T4"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "7c98df1187f341c1a4dd3c453147e336": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_6e83bfc0c7774676962f461ddd2bb4e2",
              "IPY_MODEL_1f26f280a0eb415ea4a8031ab4887d94",
              "IPY_MODEL_129eed4c28fa440aac01f13c83ea6d8b"
            ],
            "layout": "IPY_MODEL_1ee8f952fd4d498fbeb8b0668639c7e1"
          }
        },
        "6e83bfc0c7774676962f461ddd2bb4e2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_13868ee44a1545b8b20dc28f65cff35c",
            "placeholder": "​",
            "style": "IPY_MODEL_7503231ee8b8474a9832dc315ba369a6",
            "value": "Epoch 1/15:   1%"
          }
        },
        "1f26f280a0eb415ea4a8031ab4887d94": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "danger",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b6a45115d77e46589c1f89378dd8fd38",
            "max": 147,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_f3cc337af8334c0b8e31dad6b0259512",
            "value": 2
          }
        },
        "129eed4c28fa440aac01f13c83ea6d8b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0e9955a1dc984e5ab32acfadf99b6990",
            "placeholder": "​",
            "style": "IPY_MODEL_16f086e88fff4c2d9d0e42fc0bc7446d",
            "value": " 2/147 [00:32&lt;32:43, 13.54s/it]"
          }
        },
        "1ee8f952fd4d498fbeb8b0668639c7e1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "13868ee44a1545b8b20dc28f65cff35c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7503231ee8b8474a9832dc315ba369a6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "b6a45115d77e46589c1f89378dd8fd38": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f3cc337af8334c0b8e31dad6b0259512": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "0e9955a1dc984e5ab32acfadf99b6990": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "16f086e88fff4c2d9d0e42fc0bc7446d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}