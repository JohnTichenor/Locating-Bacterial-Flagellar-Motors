{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "6Tl3gy-k-3ng"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "score_threshold = 0.5"
      ],
      "metadata": {
        "id": "7yWplRji-5cg"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Utility Functions to compute F2 scores for validation"
      ],
      "metadata": {
        "id": "kFlQneRk-yrF"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f-wTsydS-qJ4"
      },
      "outputs": [],
      "source": [
        "#Utility Functions to compute F2 scores for validation\n",
        "import torch\n",
        "from tqdm import tqdm\n",
        "\n",
        "def compute_iou(boxA, boxB):\n",
        "    \"\"\"\n",
        "    Compute the Intersection-over-Union (IoU) between two bounding boxes.\n",
        "\n",
        "    Args:\n",
        "        boxA (array-like): [x1, y1, x2, y2]\n",
        "        boxB (array-like): [x1, y1, x2, y2]\n",
        "\n",
        "    Returns:\n",
        "        float: IoU value\n",
        "    \"\"\"\n",
        "    xA = max(boxA[0], boxB[0])\n",
        "    yA = max(boxA[1], boxB[1])\n",
        "    xB = min(boxA[2], boxB[2])\n",
        "    yB = min(boxA[3], boxB[3])\n",
        "\n",
        "    interArea = max(0, xB - xA) * max(0, yB - yA)\n",
        "    if interArea == 0:\n",
        "        return 0.0\n",
        "\n",
        "    boxAArea = (boxA[2] - boxA[0]) * (boxA[3] - boxA[1])\n",
        "    boxBArea = (boxB[2] - boxB[0]) * (boxB[3] - boxB[1])\n",
        "\n",
        "    iou = interArea / float(boxAArea + boxBArea - interArea)\n",
        "    return iou\n",
        "\n",
        "\n",
        "def compute_f2_score(pred_boxes, gt_boxes, iou_threshold=0.5):\n",
        "    \"\"\"\n",
        "    Compute F2 score for a single image, given predicted and ground-truth boxes.\n",
        "\n",
        "    Args:\n",
        "        pred_boxes (np.ndarray): Array of predicted boxes [N_pred, 4]\n",
        "        gt_boxes (np.ndarray): Array of ground truth boxes [N_gt, 4]\n",
        "        iou_threshold (float): IoU threshold to consider a detection a true positive\n",
        "\n",
        "    Returns:\n",
        "        float: F2 score for this image\n",
        "    \"\"\"\n",
        "    matched_gt = set()\n",
        "    tp = 0  # True positives\n",
        "\n",
        "    for pred_box in pred_boxes:\n",
        "        for i, gt_box in enumerate(gt_boxes):\n",
        "            if i in matched_gt:\n",
        "                continue\n",
        "            if compute_iou(pred_box, gt_box) >= iou_threshold:\n",
        "                tp += 1\n",
        "                matched_gt.add(i)\n",
        "                break  # Move to next predicted box\n",
        "    fp = len(pred_boxes) - tp  # False positives\n",
        "    fn = len(gt_boxes) - tp    # False negatives\n",
        "\n",
        "    beta2 = 4  # beta^2 for F2 score (beta=2)\n",
        "\n",
        "    # Avoid division by zero\n",
        "    precision = tp / (tp + fp) if (tp + fp) > 0 else 0.0\n",
        "    recall = tp / (tp + fn) if (tp + fn) > 0 else 0.0\n",
        "    if precision + recall == 0:\n",
        "        return 0.0\n",
        "    f2 = (1 + beta2) * (precision * recall) / (beta2 * precision + recall)\n",
        "    return f2"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Modified Training / Validation Loop"
      ],
      "metadata": {
        "id": "uut9gY54_Ad3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Training hyperparameters\n",
        "num_epochs = 10\n",
        "lr = 0.005\n",
        "\n",
        "# Optimizer setup\n",
        "optimizer = torch.optim.SGD(\n",
        "    model.parameters(), lr=lr, momentum=0.9, weight_decay=0.0005\n",
        ")\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    total_loss = 0.0\n",
        "    pbar = tqdm(data_loader, desc=f\"Epoch {epoch + 1}/{num_epochs}\", leave=False)\n",
        "\n",
        "    # Training step\n",
        "\n",
        "    for images, targets in pbar:\n",
        "\n",
        "        images = [img.to(device) for img in images]\n",
        "        targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
        "\n",
        "        loss_dict = model(images, targets)\n",
        "        losses = sum(loss for loss in loss_dict.values())\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        losses.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += losses.item()\n",
        "\n",
        "    print(f\"[Epoch {epoch + 1}/{num_epochs}] Loss: {total_loss:.4f}\")\n",
        "\n",
        "    # ---- Validation Step: Compute average F2 score ----\n",
        "\n",
        "    model.eval()\n",
        "    val_f2s = []\n",
        "    with torch.no_grad():\n",
        "        for images, targets in tqdm(val_loader, desc=\"Validation\", leave=False):\n",
        "            images = [img.to(device) for img in images]\n",
        "\n",
        "            # Get ground truth boxes as numpy arrays for each image in batch\n",
        "            gt_boxes_batch = [t[\"boxes\"].cpu().numpy() for t in targets]\n",
        "\n",
        "            # Inference step\n",
        "            outputs = model(images)\n",
        "\n",
        "            # Each output is a dict with \"boxes\", \"scores\", \"labels\"\n",
        "            for pred, gt_boxes in zip(outputs, gt_boxes_batch):\n",
        "                pred_boxes = pred[\"boxes\"].cpu().numpy()\n",
        "\n",
        "                # Keep detections with score > score_threshold\n",
        "\n",
        "                if \"scores\" in pred:\n",
        "                    keep = pred[\"scores\"].cpu().numpy() > score_threshold\n",
        "                    pred_boxes = pred_boxes[keep]\n",
        "\n",
        "                # Compute F2 score for this image and add to list\n",
        "                f2 = compute_f2_score(pred_boxes, gt_boxes)\n",
        "                val_f2s.append(f2)\n",
        "\n",
        "    # Compute average F2 across all validation images for this epoch\n",
        "    avg_f2 = sum(val_f2s) / len(val_f2s) if val_f2s else 0.0\n",
        "    print(f\"[Epoch {epoch + 1}/{num_epochs}] Validation F2: {avg_f2:.4f}\")"
      ],
      "metadata": {
        "id": "hw8oG6FV-_9a"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}